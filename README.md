
# ğŸ“˜ Mental Health PDF Q&A System using RAG and Ollama

This project sets up a Retrieval-Augmented Generation (RAG) pipeline to interact with a mental health-related PDF. It extracts the contents, creates embeddings, stores them in a FAISS vector store, and enables question-answering using a local LLM (`deepseek-r1:1.5b`) with fallback support if the context is insufficient.

## ğŸ› ï¸ Features

- âœ… Converts a PDF to Markdown using `docling`
- âœ… Splits the Markdown into structured chunks based on headers
- âœ… Uses `Ollama` embeddings (`nomic-embed-text`) for vector representation
- âœ… Stores and retrieves chunks using `FAISS`
- âœ… RAG setup using `LangChain` with fallback to model-generated answers if context is insufficient
- âœ… Outputs bullet-point answers

---

## ğŸ“‚ Project Structure

```
mental_health_rag/
â”‚
â”œâ”€â”€ app.py                    # Main script with RAG chain
â”œâ”€â”€ mental_health.pdf         # Source document
â”œâ”€â”€ .env                      # Environment variables (optional)
â””â”€â”€ README.md                 # Project documentation
```

---

## ğŸ”§ Installation

1. **Clone the repo:**

```bash
git clone https://github.com/yourname/mental_health_rag.git
cd mental_health_rag
```

2. **Set up a virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Run Ollama and pull models:**
```bash
ollama run nomic-embed-text
ollama run deepseek-r1:1.5b
```

---

## ğŸ“„ Required Libraries

Hereâ€™s a list of required libraries (add them in `requirements.txt`):

```
streamlit
python-dotenv
faiss-cpu
langchain
langchain-community
langchain-core
langchain-text-splitters
langchain-ollama
docling
```

---

## ğŸš€ How It Works

1. **Document Loading & Conversion:**
   - Uses `DocumentConverter` from `docling` to convert PDF to Markdown.

2. **Markdown Splitting:**
   - Uses `MarkdownHeaderTextSplitter` to chunk content by headers (`#`, `##`, `###`).

3. **Embeddings & Vector Store:**
   - Generates embeddings using `nomic-embed-text` model via Ollama.
   - FAISS stores vectors for retrieval.

4. **Retriever & Chain:**
   - Retrieves top 3 relevant chunks using MMR.
   - If context is insufficient, the model generates a fallback answer.

5. **Model Answering:**
   - Uses `deepseek-r1:1.5b` for generating answers in bullet points.

---

## ğŸ§  Example Usage

```python
question = "What is water?"
for chunk in rag_chain.stream(question):
    print(chunk, end="", flush=True)
```

---

## âœ… Output Sample

```
Question: What is water?
- Water is a vital resource for life, often referenced in mental health documents for hydration and wellness.
- It plays a role in cognitive function and emotional regulation.
- Lack of water intake can sometimes mimic or exacerbate symptoms of anxiety and fatigue.
--------------------------------------------------
```

---

## ğŸ—‚ï¸ Future Improvements

- Add a `Streamlit` UI for interactive Q&A
- Multi-file PDF support
- Answer highlighting from source
- Switch models dynamically (DeepSeek, LLaMA, etc.)

---

## ğŸ‘¤ Author

**Your Name** â€“ [GitHub](https://github.com/yourname)
